# resources for the prod namespace are defined here: https://us-east-1.console.aws.amazon.com/eks/home?region=us-east-1#/clusters/hub-prod/nodegroups/datasets-server-20220513085103612000000001
# the nodes are 4 t3.2xlarge instances (8 vCPUs, 32 GiB), ie:
# 32 vCPUs and 128 GiB RAM are available (but no more than 8 cpus or 32 GiB for each pod)
#
# the max resources (limits) per deployment are:
# - reverse-proxy: 2 pods -> 2 CPUs, 512MiB
# - api: 4 pods -> 4 CPUs, 4 GiB
# - admin: 1 pod -> 1 CPU
# this lets 25 CPUs, and 123 GiB for the workers
# we will over-commit the resources by a factor 4 in order to get more workers
# available in case of burst of jobs (most of the jobs don't require much resources)
# so: 100 CPUs and 492 GiB RAM
#
# - datasets-worker: 4 workers -> 6 CPUs, 30 GiB
# - splits-worker: 12 workers -> 6 CPUs, 30 GiB

mongodb:
  enabled: false
  # we use the secret instead to get the mongo URL

storage:
  nfs:
    path: "/fsx"
    server: "svm-0bd5fa426547fca22.fs-02050b8d555063cde.fsx.us-east-1.amazonaws.com"
    # https://us-east-1.console.aws.amazon.com/fsx/home?region=us-east-1#file-system-details/fs-02050b8d555063cde
    # Alarm: https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#alarmsV2:alarm/Low+disk+on+datasets+server?

secrets:
  hfToken: hf-token
  mongoUrl: mongo-url

monitoring:
  enabled: true

adminDomain: "datasets-server-admin.us.dev.moon.huggingface.tech"
apiDomain: "datasets-server.huggingface.co"

reverseProxy:
  replicas: 2

  ingress:
    annotations:
      alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-east-1:707930574880:certificate/777e3ae5-0c54-47ee-9b8c-d85eeb6ec4ae
      alb.ingress.kubernetes.io/healthcheck-path: "/healthcheck"
      alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80, "HTTPS": 443}]'
      alb.ingress.kubernetes.io/load-balancer-name: "hub-datasets-server-prod"
      alb.ingress.kubernetes.io/scheme: "internet-facing"
      alb.ingress.kubernetes.io/tags: "Env=prod,Project=datasets-server,Terraform=true"
      alb.ingress.kubernetes.io/target-node-labels: role-datasets-server=true
      kubernetes.io/ingress.class: "alb"

  service:
    annotations:
      service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags: Env=prod,Project=datasets-server,Terraform=true
      service.beta.kubernetes.io/aws-load-balancer-name: hub-prod-datasets-server-nlb
      service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: instance
      service.beta.kubernetes.io/aws-load-balancer-scheme: internal
      service.beta.kubernetes.io/aws-load-balancer-type: external
      service.beta.kubernetes.io/aws-load-balancer-target-node-labels: role-datasets-server=true

  nodeSelector:
    role-datasets-server: "true"

  tolerations:
    - key: CriticalAddonsOnly
      operator: Equal

  resources:
    requests:
      cpu: 0.1
      memory: "256Mi"
    limits:
      cpu: 1
      memory: "256Mi"

api:
  replicas: 4

  nodeSelector:
    role-datasets-server: "true"

  resources:
    requests:
      cpu: 0.25
      memory: "512Mi"
    limits:
      cpu: 1
      memory: "1Gi"

  appNumWorkers: "1"

datasetsWorker:
  replicas: 4

  nodeSelector:
    role-datasets-server: "true"

  resources:
    requests:
      cpu: 0.01
      memory: "1Gi"
    limits:
      cpu: 6
      memory: "30Gi"

splitsWorker:
  replicas: 10

  nodeSelector:
    role-datasets-server: "true"

  resources:
    requests:
      cpu: 0.01
      memory: "1Gi"
    limits:
      cpu: 6
      memory: "30Gi"

  # Log level
  logLevel: "DEBUG"
  # Maximum number of jobs running at the same time for the same dataset
  maxJobsPerDataset: 5

admin:
  replicas: 1

  nodeSelector:
    role-datasets-server: "true"

  resources:
    requests:
      cpu: 0.01
    limits:
      cpu: 1

  # Log level
  logLevel: "DEBUG"

  appNumWorkers: "1"
