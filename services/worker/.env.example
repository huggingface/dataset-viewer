# Assets directory
# ASSETS_DIRECTORY=

# Datasets blocklist
# DATASETS_BLOCKLIST=""

# Git reference for the canonical datasets on https://github.com/huggingface/datasets
# DATASETS_REVISION="master"

# User Access Token (see https://huggingface.co/settings/token, only the `read` role is required)
# HF_TOKEN=

# Log level
# LOG_LEVEL = "INFO"

# Maximum number of jobs running at the same time for the same dataset
# MAX_JOBS_PER_DATASET = 1

# Max CPU load (%) - if reached, sleeps until it comes back under the limit
# MAX_LOAD_PCT = 50

# Max memory (RAM + SWAP) (%) - if reached, sleeps until it comes back under the limit
# MAX_MEMORY_PCT = 60

# Max size (in bytes) of the dataset to fallback in normal mode if streaming fails
# MAX_SIZE_FALLBACK = 100_000_000

# Min size of a cell in the /rows endpoint response in bytes
# MIN_CELL_BYTES=100

# Name of the mongo db database used to cache the datasets
# MONGO_CACHE_DATABASE="datasets_server_cache"

# Name of the mongo db database used to store the jobs queue
# MONGO_QUEUE_DATABASE="datasets_server_queue"

# URL to connect to mongo db
# MONGO_URL="mongodb://localhost:27017"

# Max size of the /rows endpoint response in bytes
# ROWS_MAX_BYTES=1_000_000

# Max number of rows in the /rows endpoint response
# ROWS_MAX_NUMBER=100

# Min number of rows in the /rows endpoint response
# ROWS_MIN_NUMBER=10

# Number of seconds a worker will sleep before trying to process a new job
# WORKER_SLEEP_SECONDS = 5

# Job queue the worker will pull jobs from: 'datasets' or 'splits'
# WORKER_QUEUE = datasets
