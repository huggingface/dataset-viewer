mongodb:
  enabled: false
  useStatefulSet: true
  auth:
    enabled: false
  serviceAccount:
    create: false
  # Name of the mongo db database used to cache the datasets
  cacheDatabase: "datasets_server_cache"
  # Name of the mongo db database used to store the jobs queue
  queueDatabase: "datasets_server_queue"

# overriden by docker-images.yaml (which must be in JSON format!)
dockerImage:
  admin: ""
  api: ""
  reverseProxy: ""
  worker:
    datasets: ""
    splits: ""

storage:
  nfs: {}

secrets:
  hfToken: datasets-server-hf-token
  mongoUrl: false

monitoring:
  enabled: false

# adminDomain: "admin-datasets-server-dev.us.dev.moon.huggingface.tech"
# apiDomain: "datasets-server-dev.us.dev.moon.huggingface.tech"
uid: 1000
gid: 3000


ingress:
  annotations: {}

reverseProxy:
  replicas: 1

  service:
    type: NodePort
    annotations: {}

  resources:
    requests:
      cpu: 1
    limits:
      cpu: 1
  nodeSelector: {}
  tolerations: []

  # Directory of assets (audio files and images that will be served for the web)
  assetsDirectory: "/assets"
  # Directory of the nginx cache
  cacheDirectory: "/nginx-cache"
  readinessPort: 80
  cacheInactive: 24h
  cacheMaxSize: 1g
  cacheZoneSize: 50m
  host: localhost
  nginxTemplateFile: "nginx-templates/default.conf.template"
  openapiFile: "static-files/openapi.json"
  port: 80

api:
  replicas: 1

  service:
    type: NodePort
    annotations: {}

  resources:
    requests:
      cpu: 1
    limits:
      cpu: 1
  nodeSelector: {}
  tolerations: []

  # Directory of assets (audio files and images that will be served for the web)
  assetsDirectory: "/assets"
  readinessPort: 80
  # Application hostname - it must not be set to localhost to work in Kube!
  appHostname: "0.0.0.0"
  # Number of uvicorn workers for running the application
  appNumWorkers: "1"
  # Application endpoint port
  appPort: 80
  # External authentication URL.
  # %s will be replaced with the dataset name, for example:
  # "https://huggingface.co/api/datasets/%s/auth-check"
  # The authentication service must follow the specification in
  # https://nginx.org/en/docs/http/ngx_http_auth_request_module.html
  # and return 200, 401 or 403
  externalAuthUrl: "https://huggingface.co/api/datasets/%s/auth-check"
  # Log level
  logLevel: "INFO"
  # Number of seconds to set in the `max-age` header on data endpoints
  maxAgeLongSeconds: "120"
  # Number of seconds to set in the `max-age` header on technical endpoints
  maxAgeShortSeconds: "10"
  # Directory where the uvicorn workers will write the prometheus metrics
  # see https://github.com/prometheus/client_python#multiprocess-mode-eg-gunicorn
  prometheusMultiprocDirectory: "/tmp"

worker:
  datasets:
    replicas: 1

    resources:
      requests:
        cpu: 1
      limits:
        cpu: 1
    nodeSelector: {}
    tolerations: []

    # Directory of assets (audio files and images that will be served for the web)
    assetsDirectory: "/assets"
    # Directory of the "datasets" library cache (for the datasets, not the modules)
    cacheDirectory: "/cache"
    # Git reference for the canonical datasets on https://github.com/huggingface/datasets
    datasetsRevision: "main"
    # User Access Token (see https://huggingface.co/settings/token, only the `read` role is required)
    hfToken: ""
    # Log level
    logLevel: "INFO"
    # Max number of job retries (for 500 errors) for the same job
    maxJobRetries: 3
    # Maximum number of jobs running at the same time for the same dataset
    maxJobsPerDataset: 1
    # Max CPU load (%) - if reached, sleeps until it comes back under the limit
    maxLoadPct: 0
    # Max memory (RAM + SWAP) (%) - if reached, sleeps until it comes back under the limit
    maxMemoryPct: 0
    # Max size (in bytes) of the dataset to fallback in normal mode if streaming fails
    maxSizeFallback: "100_000_000"
    # Min size of a cell in the /rows endpoint response in bytes
    minCellBytes: 100
    # Directory of the "numba" library cache
    numbaCacheDirectory: "/numba-cache"
    # Max size of the /rows endpoint response in bytes
    rowMaxBytes: "1_000_000"
    # Max number of rows in the /rows endpoint response
    rowsMaxNumber: 100
    # Min number of rows in the /rows endpoint response
    rowsMinNumber: 10
    # Number of seconds a worker will sleep before trying to process a new job
    workerSleepSeconds: 15

  firstRows:
    replicas: 1

    resources:
      requests:
        cpu: 1
      limits:
        cpu: 1
    nodeSelector: {}
    tolerations: []

    # Directory of assets (audio files and images that will be served for the web)
    assetsDirectory: "/assets"
    # Directory of the "datasets" library cache (for the datasets, not the modules)
    cacheDirectory: "/cache"
    # Git reference for the canonical datasets on https://github.com/huggingface/datasets
    datasetsRevision: "main"
    # User Access Token (see https://huggingface.co/settings/token, only the `read` role is required)
    hfToken: ""
    # Log level
    logLevel: "INFO"
    # Max number of job retries (for 500 errors) for the same job
    maxJobRetries: 3
    # Maximum number of jobs running at the same time for the same dataset
    maxJobsPerDataset: 1
    # Max CPU load (%) - if reached, sleeps until it comes back under the limit
    maxLoadPct: 0
    # Max memory (RAM + SWAP) (%) - if reached, sleeps until it comes back under the limit
    maxMemoryPct: 0
    # Max size (in bytes) of the dataset to fallback in normal mode if streaming fails
    maxSizeFallback: "100_000_000"
    # Min size of a cell in the /rows endpoint response in bytes
    minCellBytes: 100
    # Directory of the "numba" library cache
    numbaCacheDirectory: "/numba-cache"
    # Max size of the /rows endpoint response in bytes
    rowMaxBytes: "1_000_000"
    # Max number of rows in the /rows endpoint response
    rowsMaxNumber: 100
    # Min number of rows in the /rows endpoint response
    rowsMinNumber: 10
    # Number of seconds a worker will sleep before trying to process a new job
    workerSleepSeconds: 15

  splitsNext:
    replicas: 1

    resources:
      requests:
        cpu: 1
      limits:
        cpu: 1
    nodeSelector: {}
    tolerations: []

    # Directory of assets (audio files and images that will be served for the web)
    assetsDirectory: "/assets"
    # Directory of the "datasets" library cache (for the datasets, not the modules)
    cacheDirectory: "/cache"
    # Git reference for the canonical datasets on https://github.com/huggingface/datasets
    datasetsRevision: "main"
    # Log level
    logLevel: "INFO"
    # Max number of job retries (for 500 errors) for the same job
    maxJobRetries: 3
    # Maximum number of jobs running at the same time for the same dataset
    maxJobsPerDataset: 1
    # Max CPU load (%) - if reached, sleeps until it comes back under the limit
    maxLoadPct: 0
    # Max memory (RAM + SWAP) (%) - if reached, sleeps until it comes back under the limit
    maxMemoryPct: 0
    # Max size (in bytes) of the dataset to fallback in normal mode if streaming fails
    maxSizeFallback: "100_000_000"
    # Min size of a cell in the /rows endpoint response in bytes
    minCellBytes: 100
    # Directory of the "numba" library cache
    numbaCacheDirectory: "/numba-cache"
    # Max size of the /rows endpoint response in bytes
    rowMaxBytes: "1_000_000"
    # Max number of rows in the /rows endpoint response
    rowsMaxNumber: 100
    # Min number of rows in the /rows endpoint response
    rowsMinNumber: 10
    # Number of seconds a worker will sleep before trying to process a new job
    workerSleepSeconds: 15

  splits:
    replicas: 1

    resources:
      requests:
        cpu: 1
      limits:
        cpu: 1
    nodeSelector: {}
    tolerations: []

    # Directory of assets (audio files and images that will be served for the web)
    assetsDirectory: "/assets"
    # Directory of the "datasets" library cache (for the datasets, not the modules)
    cacheDirectory: "/cache"
    # Git reference for the canonical datasets on https://github.com/huggingface/datasets
    datasetsRevision: "main"
    # Log level
    logLevel: "INFO"
    # Max number of job retries (for 500 errors) for the same job
    maxJobRetries: 3
    # Maximum number of jobs running at the same time for the same dataset
    maxJobsPerDataset: 1
    # Max CPU load (%) - if reached, sleeps until it comes back under the limit
    maxLoadPct: 0
    # Max memory (RAM + SWAP) (%) - if reached, sleeps until it comes back under the limit
    maxMemoryPct: 0
    # Max size (in bytes) of the dataset to fallback in normal mode if streaming fails
    maxSizeFallback: "100_000_000"
    # Min size of a cell in the /rows endpoint response in bytes
    minCellBytes: 100
    # Directory of the "numba" library cache
    numbaCacheDirectory: "/numba-cache"
    # Max size of the /rows endpoint response in bytes
    rowMaxBytes: "1_000_000"
    # Max number of rows in the /rows endpoint response
    rowsMaxNumber: 100
    # Min number of rows in the /rows endpoint response
    rowsMinNumber: 10
    # Number of seconds a worker will sleep before trying to process a new job
    workerSleepSeconds: 15

admin:
  replicas: 1

  service:
    type: NodePort
    annotations: {}

  resources:
    requests:
      cpu: 1
    limits:
      cpu: 1
  nodeSelector: {}
  tolerations: []

  # Application hostname - it must not be set to localhost to work in Kube!
  appHostname: "0.0.0.0"
  # Number of uvicorn workers for running the application
  appNumWorkers: "1"
  # Application endpoint port
  appPort: 80
  # Directory of assets (audio files and images that will be served for the web)
  assetsDirectory: "/assets"
  # Number of reports in /cache-reports/... endpoints
  cacheReportsNumResults: 100
  # Log level
  logLevel: "INFO"
  # Number of seconds to set in the `max-age` header on technical endpoints
  maxAgeShortSeconds: "10"
  # Directory where the uvicorn workers share their prometheus metrics
  # see https://github.com/prometheus/client_python#multiprocess-mode-eg-gunicorn
  prometheusMultiprocDirectory: "/tmp"

  readinessPort: 80
