# --- common parameters ---

hostname: "datasets-server.huggingface.co"

secrets:
  hfToken: datasets-server-hf-token
  mongoUrl: false

uid: 1000
gid: 3000

storage:
  nfs: {}

monitoring:
  enabled: false

mongodb:
  enabled: false
  useStatefulSet: true
  auth:
    enabled: false
  serviceAccount:
    create: false

# overriden by docker-images.yaml (which must be in JSON format!)
dockerImage:
  reverseProxy: ""
  services:
    admin: ""
    api: ""
  workers:
    splits: ""
    firstRows: ""

cache:
  # Directory on the shared storage (audio files and images)
  assetsDirectory: "/assets"
  # Name of the mongo db database used to cache the API responses
  mongoDatabase: "datasets_server_cache"

queue:
  # Maximum number of jobs running at the same time for the same namespace
  maxJobsPerNamespace: 1
  # Max CPU load (%) - if reached, sleeps until it comes back under the limit
  maxLoadPct: 0
  # Max memory (RAM + SWAP) (%) - if reached, sleeps until it comes back under the limit
  maxMemoryPct: 0
  # Name of the mongo db database used to store the jobs queue
  mongoDatabase: "datasets_server_queue"
  # Number of seconds a worker will sleep before trying to process a new job
  sleepSeconds: 5

common:
  # base URL for the assets files. It should be set accordingly to the datasets-server domain, eg https://datasets-server.huggingface.co/assets
  # assetsBaseUrl: "not used for now"
  # URL of the HuggingFace Hub
  hfEndpoint: "https://huggingface.co"
  # Log level
  logLevel: "INFO"

# Directory where the `datasets` library will store the cached datasets data
hfDatasetsCache: "/hf-datasets-cache"
# Directory where the `datasets` library will store the cached datasets scripts
#hfModulesCache: "not used"
# Directory where the `numba` decorators (used by `librosa`) can write cache
numbaCacheDirectory: "/numba-cache"

# --- reverse proxy ---

reverseProxy:
  host: localhost
  port: 80
  nginxTemplateFile: "nginx-templates/default.conf.template"
  openapiFile: "static-files/openapi.json"
  error404File: "nginx-templates/404.html"

  nodeSelector: {}
  readinessPort: 80
  replicas: 1
  resources:
    requests:
      cpu: 1
    limits:
      cpu: 1
  service:
    type: NodePort
    annotations: {}
  tolerations: []

ingress:
  annotations:
    alb.ingress.kubernetes.io/healthcheck-path: "/healthcheck"
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80, "HTTPS": 443}]'
    alb.ingress.kubernetes.io/scheme: "internet-facing"
    alb.ingress.kubernetes.io/group.name: "datasets-server"
    kubernetes.io/ingress.class: "alb"

# --- services ---

admin:
  # HF organization that is allowed to request the report
  hfOrganization: "huggingface"
  # Number of reports in /cache-reports/... endpoints
  cacheReportsNumResults: 100
  # The path of the whoami service on the hub.
  hfWhoamiPath: "/api/whoami-v2"
  # Number of seconds to set in the `max-age` header on technical endpoints
  maxAge: "10"
  # Directory where the uvicorn workers share their prometheus metrics
  # see https://github.com/prometheus/client_python#multiprocess-mode-eg-gunicorn
  prometheusMultiprocDirectory: "/tmp"
  # hostname - it must not be set to localhost to work in Kube!
  uvicornHostname: "0.0.0.0"
  # Number of uvicorn workers for running the application
  uvicornNumWorkers: "1"
  # Application endpoint port
  uvicornPort: 80

  nodeSelector: {}
  readinessPort: 80
  replicas: 1
  resources:
    requests:
      cpu: 1
    limits:
      cpu: 1
  service:
    type: NodePort
    annotations: {}
  tolerations: []

api:
  # the path of the external authentication service on the hub.
  # The string must contain `%s` which will be replaced with the dataset name.
  hfAuthPath: "/api/datasets/%s/auth-check"
  # Number of seconds to set in the `max-age` header on data endpoints
  maxAgeLong: "120"
  # Number of seconds to set in the `max-age` header on technical endpoints
  maxAgeShort: "10"
  # Directory where the uvicorn workers will write the prometheus metrics
  # see https://github.com/prometheus/client_python#multiprocess-mode-eg-gunicorn
  prometheusMultiprocDirectory: "/tmp"
  # Hostname - it must not be set to localhost to work in Kube!
  uvicornHostname: "0.0.0.0"
  # Number of uvicorn workers for running the application
  uvicornNumWorkers: "1"
  # Application endpoint port
  uvicornPort: 80

  nodeSelector: {}
  readinessPort: 80
  replicas: 1
  resources:
    requests:
      cpu: 1
    limits:
      cpu: 1
  service:
    type: NodePort
    annotations: {}
  tolerations: []

# --- workers ---

splits:
  # override the common queue parameters
  queue:
    # Maximum number of jobs running at the same time for the same namespace
    maxJobsPerNamespace: 1

  nodeSelector: {}
  replicas: 1
  resources:
    requests:
      cpu: 1
    limits:
      cpu: 1
  tolerations: []

firstRows:
  # Max size (in bytes) of the dataset to fallback in normal mode if streaming fails
  fallbackMaxDatasetSize: "100_000_000"
  # Max size of the /first-rows endpoint response in bytes
  maxBytes: "1_000_000"
  # Max number of rows in the /first-rows endpoint response
  maxNumber: 100
  # Min size of a cell in the /first-rows endpoint response in bytes
  minCellBytes: 100
  # Min number of rows in the /first-rows endpoint response
  minNumber: 10
  # override the common queue parameters
  queue:
    # Maximum number of jobs running at the same time for the same namespace
    maxJobsPerNamespace: 1

  nodeSelector: {}
  replicas: 1
  resources:
    requests:
      cpu: 1
    limits:
      cpu: 1
  tolerations: []
