# SPDX-License-Identifier: Apache-2.0
# Copyright 2022 The HuggingFace Authors.

## Production
#
# datasets-server is installed on a [kubernetes cluster](https://us-east-1.console.aws.amazon.com/eks/home?region=us-east-1#/clusters)
#
# Grafana:
#
# - https://grafana.huggingface.tech/d/SaHl2KX7z/datasets-server-admin-cache-and-queue
# - https://grafana.huggingface.tech/d/iPuzZbrnk/datasets-server-api-routes
# - https://grafana.huggingface.tech/d/85a562078cdf77779eaa1add43ccec1e/kubernetes-compute-resources-namespace-pods?var-datasource=Prometheus%20EKS%20Hub%20Prod&var-namespace=datasets-server
#
# BetterUptime:
#
# - https://betteruptime.com/team/14149/monitors/389098
# - https://betteruptime.com/team/14149/monitors/691070
#
# resources for the prod namespace are defined here: https://us-east-1.console.aws.amazon.com/eks/home?region=us-east-1#/clusters/hub-prod/nodegroups/datasets-server-20220513085103612000000001
# the nodes are up to 20 t3.2xlarge instances (8 vCPUs, 32 GiB), with autoscale
# (see https://github.com/huggingface/infra/pull/239/files)
# this means that we can get up to:
# 160 vCPUs and 640 GiB RAM are available (but no more than 8 cpus or 32 GiB for each pod)
#
# the max resources (limits) per deployment are:
# - reverse-proxy: 2 pods -> 2 CPUs, 512MiB
# - api: 4 pods -> 4 CPUs, 4 GiB
# - admin: 1 pod -> 1 CPU
# and for the workers:
# - splits: 1 CPUs, 30 GiB
# - firstRows: 1 CPUs, 30 GiB
# We set the requested RAM to 8 GiB per worker, in order to trigger the autoscale. We should be able to
# launch 3 worker pods per node, taking the sidecars into account, it means 60 pods
#
# Being optimistic about not all the pods having to increase their memory usage to 30 GiB at the same time,
# ie over-committing a bit, we can set up to 60 workers (dataset + split).
#
# For now, we have to scale manually with:
#  kubectl scale --replicas=16 deploy/datasets-server-prod-worker-splits
# or
#  kubectl scale --replicas=32 deploy/datasets-server-prod-worker-first-rows

# --- common parameters ---

hostname: "datasets-server.huggingface.co"

secrets:
  mongoUrl:
    fromSecret: true
    secretName: "mongo-url"
    value: mongo://
  appHfToken:
    fromSecret: true
    secretName: "hf-token"
  userHfToken:
    fromSecret: true
    secretName: "hf-token-francky"

persistence:
  # https://us-east-1.console.aws.amazon.com/fsx/home?region=us-east-1#file-system-details/fs-02050b8d555063cde
  # Alarm: https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#alarmsV2:alarm/Low+disk+on+datasets+server?
  existingClaim: "nfs-datasets-server-pvc"

monitoring:
  enabled: true

mongodb:
  # we use the secret instead to get the mongo URL
  enabled: false

common:
  # Log level
  logLevel: "DEBUG"

# --- jobs (pre-install/upgrade hooks) ---

mongodbMigration:
  nodeSelector:
    role-datasets-server: "true"

# --- storage admin (to manually inspect the storage, in /data) ---

storageAdmin:
  nodeSelector:
    role-datasets-server: "true"
  replicas: 1
  resources:
    requests:
      cpu: 1
      memory: "256Mi"
    limits:
      cpu: 1
      memory: "256Mi"

# --- reverse proxy ---

reverseProxy:
  nodeSelector:
    role-datasets-server: "true"
  replicas: 2
  resources:
    requests:
      cpu: 1
      memory: "256Mi"
    limits:
      cpu: 1
      memory: "256Mi"
  service:
    annotations:
      service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags: Env=prod,Project=datasets-server,Terraform=true
      service.beta.kubernetes.io/aws-load-balancer-name: hub-prod-datasets-server-nlb
      service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: instance
      service.beta.kubernetes.io/aws-load-balancer-scheme: internal
      service.beta.kubernetes.io/aws-load-balancer-type: external
      service.beta.kubernetes.io/aws-load-balancer-target-node-labels: role-datasets-server=true
  tolerations:
    - key: CriticalAddonsOnly
      operator: Equal

ingress:
  annotations:
    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-east-1:707930574880:certificate/971187a3-2baa-40e5-bcae-94d6ec55cd24
    alb.ingress.kubernetes.io/load-balancer-name: "hub-datasets-server-prod"
    alb.ingress.kubernetes.io/tags: "Env=prod,Project=datasets-server,Terraform=true"
    alb.ingress.kubernetes.io/target-node-labels: role-datasets-server=true
    alb.ingress.kubernetes.io/healthcheck-path: "/healthcheck"
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80, "HTTPS": 443}]'
    alb.ingress.kubernetes.io/scheme: "internet-facing"
    alb.ingress.kubernetes.io/group.name: "datasets-server"
    kubernetes.io/ingress.class: "alb"

# --- services ---

admin:
  # Number of reports in /cache-reports/... endpoints
  cacheReportsNumResults: 1000
  # Number of uvicorn workers for running the application
  # (2 x $num_cores) + 1
  # https://docs.gunicorn.org/en/stable/design.html#how-many-workers
  uvicornNumWorkers: "9"

  nodeSelector:
    role-datasets-server: "true"
  replicas: 1
  resources:
    requests:
      cpu: 4
      memory: "512Mi"
    limits:
      cpu: 4
      memory: "4Gi"

api:
  # Number of uvicorn workers for running the application
  # (2 x $num_cores) + 1
  # https://docs.gunicorn.org/en/stable/design.html#how-many-workers
  uvicornNumWorkers: "9"

  nodeSelector:
    role-datasets-server: "true"
  replicas: 4
  resources:
    requests:
      cpu: 4
      memory: "512Mi"
    limits:
      cpu: 4
      memory: "4Gi"

# --- workers ---

splits:
  # override the common queue parameters
  queue:
    # Maximum number of jobs running at the same time for the same namespace
    maxJobsPerNamespace: 1

  nodeSelector:
    role-datasets-server: "true"
  replicas: 12
  resources:
    requests:
      cpu: 1
      memory: "8Gi"
    limits:
      cpu: 2
      memory: "30Gi"

firstRows:
  # Max size of the /first-rows endpoint response in bytes
  maxBytes: "200_000"

  # override the common queue parameters
  queue:
    # Maximum number of jobs running at the same time for the same namespace
    maxJobsPerNamespace: 4

  nodeSelector:
    role-datasets-server: "true"
  replicas: 16
  resources:
    requests:
      cpu: 1
      memory: "8Gi"
    limits:
      cpu: 2
      memory: "30Gi"

parquet:
  # comma-separated list of the blocked datasets. Defaults to empty.
  blockedDatasets: "matallanas/linustechtips-transcript-audio-wav,KnutJaegersberg/Interpretable_word_embeddings_large_cskg,ashraf-ali/quran-data,cjvt/cc_gigafida,cmudrc/porous-microstructure-strain-fields,dlwh/MultiLegalPile_Wikipedia_Shuffled,izumaru/os2-datasets,joelito/MultiLegalPile_Wikipedia_Filtered,leviethoang/VBVLSP,nyanko7/yandere-images,severo/wit,texturedesign/td01_natural-ground-textures,Tristan/olm-october-2022-tokenized-1024-exact-dedup-only,Whispering-GPT/linustechtips-transcript-audio,beyond/chinese_clean_passages_80m,bigscience/xP3,dalle-mini/YFCC100M_OpenAI_subset,galman33/gal_yair_166000_256x256_fixed,matallanas/linustechtips-transcript-audio-mp3,mwitiderrick/arXiv,sjpmpzx/qm_ly_gy_soundn,tilos/ASR-CCANTCSC,matallanas/linustechtips-transcript-audio-ogg,bigcode/the-stack,VIMA/VIMA-Data,severo/wit,wmt/europarl,chrisjay/mnist-adversarial-dataset,mwitiderrick/arXiv,HuggingFaceM4/TextCaps,CristianaLazar/librispeech5k_train,texturedesign/td01_natural-ground-textures,cjvt/cc_gigafida,Yehor/ukrainian-tts-lada,YWjimmy/PeRFception-v1,SDbiaseval/dataset-dalle,Pinguin/images,DTU54DL/librispeech5k-augmentated-train-prepared,CristianaLazar/librispeech500,abdusahmbzuai/masc_dev,anonymousdeepcc/DeepCC,bigcode/the-stack-username-to-repo,bigscience/massive-probing-results,dgrnd4/stanford_dog_dataset,gigant/romanian_speech_synthesis_0_8_1,helena-balabin/sentences,icelab/ntrs_meta,joefox/Mozilla_Common_Voice_ru_test_noise,m-aliabbas/idrak_splitted_amy_1,marinone94/nst_sv,mbarnig/lb-de-fr-en-pt-12800-TTS-CORPUS,momilla/Ethereum_transacitons,nev/anime-giph,openclimatefix/nimrod-uk-1km-validation,raghav66/whisper-gpt,strombergnlp/broad_twitter_corpus,z-uo/female-LJSpeech-italian,Champion/vpc2020_clear_anon_speech,DelgadoPanadero/Pokemon,GEM/references,HuggingFaceM4/FairFace,Karavet/ILUR-news-text-classification-corpus,Voicemod/LibriTTS-100-preproc,YWjimmy/PeRFception-v1-1,albertvillanova/TextCaps,allenai/c4,dog/punks,chenghao/scielo_books,YWjimmy/PeRFception-v1-2,bigcode/the-stack-dedup,openclimatefix/era5,Carlisle/msmarco-passage-non-abs,SetFit/mnli,valurank/PoliticalBias_AllSides_Txt,Biomedical-TeMU/ProfNER_corpus_classification,LeoFeng/MLHW_6,pragnakalp/squad_v2_french_translated,textvqa,polinaeterna/vox_lingua,nishita/ade20k-sample,oyk100/ChaSES-data,YWjimmy/PeRFception-v1-3,YWjimmy/PeRFception-ScanNet,ChaiML/AnthropicRLHFPreferenceData,voidful/librispeech_asr_text,Isma/librispeech_1000_seed_42,Graphcore/vqa-lxmert,Tevatron/wikipedia-curated-corpus,adamlin/daily_dialog,cameronbc/synthtiger,clarin-pl/multiwiki_90k,echarlaix/vqa-lxmert,gigant/african_accented_french,Graphcore/vqa,echarlaix/vqa,jimregan/clarinpl_studio,GEM/xsum,Tevatron/wikipedia-squad-corpus,mulcyber/europarl-mono,nateraw/wit,bigscience/P3,tau/mrqa,uva-irlab/trec-cast-2019-multi-turn,vblagoje/wikipedia_snippets_streamed,Tevatron/wikipedia-wq-corpus,malteos/paperswithcode-aspects,Samip/Scotch,iluvvatar/RuREBus,nateraw/quickdraw,tau/scrolls,qanastek/MASSIVE,TalTechNLP/VoxLingua107,shanya/crd3,HugoLaurencon/libri_light,jerpint/imagenette,Leyo/TGIF,DFKI-SLT/few-nerd,crystina-z/msmarco-passage-dl20,HuggingFaceM4/epic_kitchens_100,HuggingFaceM4/yttemporal180m,andreagasparini/librispeech_train_other_only,allenai/nllb,biglam/nls_chapbook_illustrations,winvoker/lvis,Lacito/pangloss,indonesian-nlp/librivox-indonesia,Graphcore/gqa-lxmert,nanom/splittedspanish3bwc,cahya/librivox-indonesia,asapp/slue,sil-ai/audio-keyword-spotting,tner/wikiann,rogerdehe/xfund,arpelarpe/nota,mwhanna/ACT-Thor,sanchit-gandhi/librispeech_asr_clean,echarlaix/gqa-lxmert,shunk031/cocostuff,gigant/m-ailabs_speech_dataset_fr,jimregan/clarinpl_sejmsenat,1aurent/icdar-2011,marinone94/nst_no,jamescalam/unsplash-25k-images,stas/openwebtext-10k,florianbussmann/train_tickets-yu2020pick,benschill/brain-tumor-collection,imvladikon/paranames,PolyAI/evi,bengaliAI/cvbn,Sreyan88/librispeech_asr,superb,mozilla-foundation/common_voice_10_0,darkproger/librispeech_asr,kresnik/librispeech_asr_test,Lehrig/Monkey-Species-Collection,HuggingFaceM4/TGIF,crystina-z/miracl-bm25-negative,cats_vs_dogs,biglam/gallica_literary_fictions,common_language,competition_math,cornell_movie_dialog,evidence_infer_treatment,hebrew_projectbenyehuda,lj_speech,mc4,muchocine,opus_euconst,tab_fact,the_pile,tapaco,turkic_xwmt,web_nlg,vctk,mathaillah/BeritaHoaks-NonHoaks,universal_morphologies,LanceaKing/asvspoof2019,andreagasparini/librispeech_train_clean_only,nuprl/MultiPL-E,SLPL/naab-raw,mteb/results,SocialGrep/the-reddit-climate-change-dataset,bigscience-biomedical/anat_em,crystina-z/xor-tydi-corpus,qanastek/QUAERO,TomTBT/pmc_open_access_section,jamescalam/movielens-25m-ratings,HuggingFaceM4/charades,Tevatron/xor-tydi-corpus,khalidalt/tydiqa-primary,nvm472001/cvdataset-layoutlmv3,Lehrig/GTZAN-Collection,mteb/tatoeba-bitext-mining,sled-umich/Action-Effect,HamdiJr/Egyptian_hieroglyphs,joelito/lextreme,cooleel/xfund_de,oscar,mozilla-foundation/common_voice_7_0,KETI-AIR/vqa,Livingwithmachines/MapReader_Data_SIGSPATIAL_2022,NLPC-UOM/document_alignment_dataset-Sinhala-Tamil-English,miracl/miracl,Muennighoff/flores200,Murple/mmcrsc,mesolitica/dbp,CodedotAI/code_clippy,keshan/clean-si-mc4,yhavinga/ccmatrix,metashift,google/fleurs,HugoLaurencon/libri_light_bytes,biwi_kinect_head_pose,ami,bigscience-biomedical/ebm_pico,HuggingFaceM4/general-pmd-synthetic-testing,crystina-z/mmarco,robertmyers/pile_v2,bigbio/anat_em,biglam/early_printed_books_font_detection,nateraw/imagenet-sketch,jpwahle/dblp-discovery-dataset,andreagasparini/librispeech_test_only,crystina-z/mmarco-corpus,mozilla-foundation/common_voice_6_0,biglam/brill_iconclass,bigscience-biomedical/evidence_inference,HuggingFaceM4/cm4-synthetic-testing,SocialGrep/ten-million-reddit-answers"
  # comma-separated list of the supported datasets. If empty, all the datasets are processed. Defaults to empty.
  supportedDatasets: ""
  # the maximum size of the supported datasets. Bigger datasets, or datasets that cannot provide the size, are ignored.
  maxDatasetSize: "5_000_000_000" # support up to 5 GB
  # override the common queue parameters
  queue:
    # Maximum number of jobs running at the same time for the same namespace
    maxJobsPerNamespace: 2
  nodeSelector:
    role-datasets-server: "true"
  replicas: 16
  resources:
    requests:
      cpu: 1
      memory: "8Gi"
    limits:
      cpu: 2
      memory: "30Gi"
  tolerations: []
