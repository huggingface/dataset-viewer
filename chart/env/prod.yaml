# SPDX-License-Identifier: Apache-2.0
# Copyright 2022 The HuggingFace Authors.

# --- common parameters ---

global:
  huggingface:
    service:
      ports:
        datasetsServer:
          proxy: 30931
          admin: 32702
          api: 31370
          rows: 31371
          search: 31372
          sseApi: 31373

images:
  pullPolicy: IfNotPresent
  pullSecrets: []
  reverseProxy:
    useGlobalRegistry: false
    registry: docker.io
    repository: nginx
    tag: "1.20"
  jobs:
    mongodbMigration:
      registry: huggingface
      useGlobalRegistry: false
      repository: datasets-server-jobs-mongodb_migration
      tag: sha-fb3399a
    cacheMaintenance:
      registry: huggingface
      useGlobalRegistry: false
      repository: datasets-server-jobs-cache_maintenance
      tag: sha-fb3399a
  services:
    admin:
      registry: huggingface
      useGlobalRegistry: false
      repository: datasets-server-services-admin
      tag: sha-fb3399a
    api:
      registry: huggingface
      useGlobalRegistry: false
      repository: datasets-server-services-api
      tag: sha-fb3399a
    rows:
      registry: huggingface
      useGlobalRegistry: false
      repository: datasets-server-services-rows
      tag: sha-fb3399a
    search:
      registry: huggingface
      useGlobalRegistry: false
      repository: datasets-server-services-search
      tag: sha-fb3399a
    sseApi:
      registry: huggingface
      useGlobalRegistry: false
      repository: datasets-server-services-sse-api
      tag: sha-fb3399a
    storageAdmin:
      registry: huggingface
      useGlobalRegistry: false
      repository: datasets-server-services-storage-admin
      tag: sha-fb3399a
    worker:
      registry: huggingface
      useGlobalRegistry: false
      repository: datasets-server-services-worker
      tag: sha-fb3399a

secrets:
  externalSecret:
    enabled: true
    secretName: "datasets-server-prod-secrets"
    secretStoreName: "datasets-server-prod-secretstore"
    parameters:
      MONGO_URL: "hub-prod-datasets-server-mongo-url"
      HF_TOKEN: "hub-prod-datasets-server-hf-token"
      PARQUET_CONVERTER_HF_TOKEN: "hub-prod-datasets-server-parquet-converter-hf-token"
      WEBHOOK_SECRET: "hub-prod-datasets-server-webhook-secret"
      SPAWNING_TOKEN: "hub-prod-datasets-server-spawning-token"
      API_HF_JWT_ADDITIONAL_PUBLIC_KEYS: "hub-prod-datasets-server-jwt-additional-public-keys"
      AWS_ACCESS_KEY_ID: "hub-prod-datasets-server-s3-access-key-id"
      AWS_SECRET_ACCESS_KEY: "hub-prod-datasets-server-s3-secret-access-key"
  mongoUrl:
    fromSecret: true
    secretName: "datasets-server-prod-secrets"
  appHfToken:
    fromSecret: true
    secretName: "datasets-server-prod-secrets"
  appParquetConverterHfToken:
    fromSecret: true
    secretName: "datasets-server-prod-secrets"
  hfWebhookSecret:
    fromSecret: true
    secretName: "datasets-server-prod-secrets"
  hfJwtAdditionalPublicKeys:
    fromSecret: true
    secretName: "datasets-server-prod-secrets"
  spawningToken:
    fromSecret: true
    secretName: "datasets-server-prod-secrets"
  s3:
    accessKeyId:
      fromSecret: true
      secretName: "datasets-server-prod-secrets"
    secretAccessKey:
      fromSecret: true
      secretName: "datasets-server-prod-secrets"

persistence:
  cachedAssets:
    existingClaim: "datasets-server-cached-assets-pvc"
  descriptiveStatistics:
    existingClaim: "datasets-server-statistics-pvc"
  duckDBIndex:
    existingClaim: "datasets-server-duckdb-pvc"
  hfDatasetsCache:
    existingClaim: "datasets-server-cache-pvc"
  nfs:
    existingClaim: "nfs-datasets-server-pvc"
  parquetMetadata:
    existingClaim: "datasets-server-parquet-pvc"

monitoring:
  enabled: true

mongodb:
  enabled: false

common:
  # URL of the HuggingFace Hub
  hfEndpoint: "https://huggingface.co"

log:
  # Log level
  level: "INFO"

firstRows:
  maxBytes: "200_000"

parquetAndInfo:
  maxDatasetSize: "5_000_000_000"
  maxRowGroupByteSizeForCopy: "300_000_000"
  noMaxSizeLimitDatasets: "Open-Orca/OpenOrca"

optInOutUrlsScan:
  maxConcurrentRequestsNumber: 100
  maxRequestsPerSecond: 50
  rowsMaxNumber: 100_000
  urlsNumberPerBatch: 1000

duckDBIndex:
  maxParquetSizeBytes: "5_000_000_000"
  expiredTimeIntervalSeconds: 259_200 # 3 days

rowsIndex:
  maxArrowDataInMemory: "300_000_000"

descriptiveStatistics:
  maxParquetSizeBytes: "5_000_000_000"

hfDatasetsCache:
  expiredTimeIntervalSeconds: 10800 # 3 hours

# --- jobs (pre-install/upgrade hooks) ---

mongodbMigration:
  nodeSelector:
    role-datasets-server: "true"
  resources:
    requests:
      cpu: 1
    limits:
      cpu: 1

# --- jobs (post-upgrade hooks) ---

cacheMaintenance:
  action: "skip"
  # ^ allowed values are {backfill, collect-queue-metrics, collect-cache-metrics, delete-indexes, post-messages, skip}
  log:
    level: "debug"
  backfill:
    error_codes_to_retry: ""
  resources:
    requests:
      cpu: 1
      memory: "4Gi"
    limits:
      cpu: 2
      memory: "8Gi"

# --- cron jobs  ---
backfill:
  enabled: true
  log:
    level: "debug"
  action: "backfill"
  error_codes_to_retry: "CreateCommitError,LockedDatasetTimeoutError,NoIndexableColumnsError"
  schedule: "20 21 * * *"
  # every four hours
  nodeSelector: {}
  resources:
    requests:
      cpu: 1
      memory: "4Gi"
    limits:
      cpu: 2
      memory: "8Gi"
  tolerations: []

queueMetricsCollector:
  action: "collect-queue-metrics"
  schedule: "*/10 * * * *"
  # every ten minutes, then it will be changed to default
  nodeSelector: {}
  resources:
    requests:
      cpu: 1
    limits:
      cpu: 1
      memory: "512Mi"
  tolerations: []

# --- storage admin (to manually inspect the storage, in /data) ---

storageAdmin:
  nodeSelector:
    role-datasets-server: "true"
  replicas: 1
  resources:
    requests:
      cpu: 4
      memory: "4Gi"
    limits:
      cpu: 4
      memory: "4Gi"

# --- reverse proxy ---

reverseProxy:
  nodeSelector:
    role-datasets-server: "true"
  replicas: 2
  resources:
    requests:
      cpu: 1
      memory: "64Mi"
    limits:
      cpu: 1
      memory: "256Mi"
  service:
    type: NodePort
  tolerations:
    - key: CriticalAddonsOnly
      operator: Equal

ingress:
  tls:
    - hosts:
        - "datasets-server.huggingface.co"
  annotations:
    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-east-1:707930574880:certificate/971187a3-2baa-40e5-bcae-94d6ec55cd24
    alb.ingress.kubernetes.io/load-balancer-name: "hub-datasets-server-prod"
    alb.ingress.kubernetes.io/tags: "Env=prod,Project=datasets-server,Terraform=true"
    alb.ingress.kubernetes.io/target-node-labels: role-datasets-server=true
    alb.ingress.kubernetes.io/healthcheck-path: "/healthcheck"
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80, "HTTPS": 443}]'
    alb.ingress.kubernetes.io/scheme: "internet-facing"
    alb.ingress.kubernetes.io/group.name: "datasets-server"
    kubernetes.io/ingress.class: "alb"

# --- services ---

admin:
  # Number of reports in /cache-reports/... endpoints
  cacheReportsNumResults: 1000
  # Number of reports in /cache-reports-with-content/... endpoints
  cacheReportsWithContentNumResults: 100
  # the timeout in seconds for the requests to the Hugging Face Hub.
  hfTimeoutSeconds: "1.5"
  # Number of uvicorn workers for running the application
  # (2 x $num_cores) + 1
  # https://docs.gunicorn.org/en/stable/design.html#how-many-workers
  uvicornNumWorkers: "9"

  nodeSelector:
    role-datasets-server: "true"
  replicas: 2
  service:
    type: NodePort
  resources:
    requests:
      cpu: 1
      memory: "8Gi"
    limits:
      cpu: 4
      memory: "8Gi"

hf:
  timeoutSeconds: "1.5"

api:
  # Number of uvicorn workers for running the application
  # (2 x $num_cores) + 1
  # https://docs.gunicorn.org/en/stable/design.html#how-many-workers
  uvicornNumWorkers: "9"

  nodeSelector:
    role-datasets-server: "true"
  replicas: 4
  service:
    type: NodePort
  resources:
    requests:
      cpu: 1
      memory: "512Mi"
    limits:
      cpu: 4
      memory: "4Gi"

rows:
  # Number of uvicorn workers for running the application
  # (2 x $num_cores) + 1
  # https://docs.gunicorn.org/en/stable/design.html#how-many-workers
  # but we only set to 2 to avoid OOM
  uvicornNumWorkers: "2"
  nodeSelector:
    role-datasets-server-rows: "true"
  replicas: 12
  service:
    type: NodePort
  resources:
    requests:
      cpu: 1
      memory: "6500Mi"
    limits:
      cpu: 4
      memory: "6500Mi"

search:
  # Number of uvicorn workers for running the application
  # (2 x $num_cores) + 1
  # https://docs.gunicorn.org/en/stable/design.html#how-many-workers
  uvicornNumWorkers: "9"
  nodeSelector:
    role-datasets-server-search: "true"
  replicas: 8
  service:
    type: NodePort
  resources:
    requests:
      cpu: 1
      memory: "6500Mi"
    limits:
      cpu: 16
      memory: "6500Mi"

sseApi:
  # Number of uvicorn workers for running the application
  # (2 x $num_cores) + 1
  # https://docs.gunicorn.org/en/stable/design.html#how-many-workers
  uvicornNumWorkers: "1"

  nodeSelector:
    role-datasets-server: "true"
  replicas: 2
  service:
    type: NodePort
  resources:
    requests:
      cpu: 1
      memory: "512Mi"
    limits:
      cpu: 4
      memory: "4Gi"

workers:
  - deployName: "heavy"
    workerDifficultyMax: 100
    workerDifficultyMin: 0
    workerJobTypesBlocked: ""
    workerJobTypesOnly: ""
    nodeSelector:
      role-datasets-server-worker: "true"
    replicas: 4
    resources:
      requests:
        cpu: 2
        memory: "34Gi"
      limits:
        cpu: 2
        memory: "34Gi"
    tolerations: []
  - deployName: "medium"
    workerDifficultyMax: 70
    workerDifficultyMin: 0
    workerJobTypesBlocked: ""
    workerJobTypesOnly: ""
    nodeSelector:
      role-datasets-server-worker: "true"
    replicas: 20
    resources:
      requests:
        cpu: 1
        memory: "14Gi"
      limits:
        cpu: 2
        memory: "14Gi"
    tolerations: []
  - deployName: "light"
    workerDifficultyMax: 40
    workerDifficultyMin: 0
    workerJobTypesBlocked: ""
    workerJobTypesOnly: ""
    nodeSelector:
      role-datasets-server-worker-light: "true"
    replicas: 10
    resources:
      requests:
        cpu: 200m
        memory: "100Mi"
      limits:
        cpu: 2
        memory: "1Gi"
    tolerations: []
